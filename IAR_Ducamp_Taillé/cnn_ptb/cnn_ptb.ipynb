{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Achitecture 1 : \"Learning Image Matching by Simply Watching Video\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named cv2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-e91ac6650005>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mabsolute_import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named cv2"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import time\n",
    "import cv2\n",
    "import sys\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import collections\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import reader\n",
    "\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "_conv_layers = [6,96,96,128,128,128]\n",
    "_activation = 'sigmoid'\n",
    "\n",
    "_batch_size = 32 \n",
    "_learning_rate = 0.05\n",
    "_epochs = 20\n",
    "_step_test = 100\n",
    "_step_viz = 100\n",
    "\n",
    "_dataset = \"ptb.train.txt\"\n",
    "\n",
    "_data_folder = \"data/\"\n",
    "_train_folder = \"train/\"\n",
    "_previously_trained = True\n",
    "\n",
    "if not os.path.exists(_train_folder):\n",
    "    os.makedirs(_train_folder)\n",
    "    print(\"Directory created :\", _train_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def activation(x,name=None):\n",
    "    if _activation == 'sigmoid':\n",
    "        return tf.nn.sigmoid(x,name)\n",
    "    if _activation == 'relu':\n",
    "        return tf.nn.relu(x,name)\n",
    "  \n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial, name='weights')\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial, name='bias')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create training batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def _read_words(filename):\n",
    "    with tf.gfile.GFile(filename, \"r\") as f:\n",
    "        return f.read().replace(\"\\n\", \"<eos>\").split()\n",
    "\n",
    "\n",
    "def _build_vocab(filename):\n",
    "    data = _read_words(filename)\n",
    "\n",
    "    counter = collections.Counter(data)\n",
    "    count_pairs = sorted(counter.items(), key=lambda x: (-x[1], x[0]))\n",
    "\n",
    "    words, _ = list(zip(*count_pairs))\n",
    "    word_to_id = dict(zip(words, range(len(words))))\n",
    "\n",
    "    return word_to_id\n",
    "\n",
    "\n",
    "def _file_to_word_ids(filename, word_to_id):\n",
    "    data = _read_words(filename)\n",
    "    return [word_to_id[word] for word in data]\n",
    "\n",
    "\n",
    "def ptb_raw_data(data_path=None):\n",
    "    \"\"\"Load PTB raw data from data directory \"data_path\".\n",
    "\n",
    "    Reads PTB text files, converts strings to integer ids,\n",
    "    and performs mini-batching of the inputs.\n",
    "\n",
    "    The PTB dataset comes from Tomas Mikolov's webpage:\n",
    "\n",
    "    http://www.fit.vutbr.cz/~imikolov/rnnlm/simple-examples.tgz\n",
    "\n",
    "    Args:\n",
    "    data_path: string path to the directory where simple-examples.tgz has\n",
    "    been extracted.\n",
    "\n",
    "    Returns:\n",
    "    tuple (train_data, valid_data, test_data, vocabulary)\n",
    "    where each of the data objects can be passed to PTBIterator.\n",
    "    \"\"\"\n",
    "\n",
    "    train_path = os.path.join(data_path, \"ptb.train.txt\")\n",
    "    valid_path = os.path.join(data_path, \"ptb.valid.txt\")\n",
    "    test_path = os.path.join(data_path, \"ptb.test.txt\")\n",
    "\n",
    "    word_to_id = _build_vocab(train_path)\n",
    "    train_data = _file_to_word_ids(train_path, word_to_id)\n",
    "    valid_data = _file_to_word_ids(valid_path, word_to_id)\n",
    "    test_data = _file_to_word_ids(test_path, word_to_id)\n",
    "    vocabulary = len(word_to_id)\n",
    "    return train_data, valid_data, test_data, vocabulary\n",
    "\n",
    "\n",
    "def ptb_iterator(raw_data, batch_size, num_steps):\n",
    "    \"\"\"Iterate on the raw PTB data.\n",
    "\n",
    "    This generates batch_size pointers into the raw PTB data, and allows\n",
    "    minibatch iteration along these pointers.\n",
    "\n",
    "    Args:\n",
    "    raw_data: one of the raw data outputs from ptb_raw_data.\n",
    "    batch_size: int, the batch size.\n",
    "    num_steps: int, the number of unrolls.\n",
    "\n",
    "    Yields:\n",
    "    Pairs of the batched data, each a matrix of shape [batch_size, num_steps].\n",
    "    The second element of the tuple is the same data time-shifted to the\n",
    "    right by one.\n",
    "\n",
    "    Raises:\n",
    "    ValueError: if batch_size or num_steps are too high.\n",
    "    \"\"\"\n",
    "    raw_data = np.array(raw_data, dtype=np.int32)\n",
    "\n",
    "    data_len = len(raw_data)\n",
    "    batch_len = data_len // batch_size\n",
    "    data = np.zeros([batch_size, batch_len], dtype=np.int32)\n",
    "    for i in range(batch_size):\n",
    "        data[i] = raw_data[batch_len * i:batch_len * (i + 1)]\n",
    "\n",
    "    epoch_size = (batch_len - 1) // num_steps\n",
    "\n",
    "    if epoch_size == 0:\n",
    "        raise ValueError(\"epoch_size == 0, decrease batch_size or num_steps\")\n",
    "\n",
    "    for i in range(epoch_size):\n",
    "        x = data[:, i*num_steps:(i+1)*num_steps]\n",
    "        y = data[:, i*num_steps+1:(i+1)*num_steps+1]\n",
    "        yield (x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Network graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "raw_data = ptb_raw_data(_data_folder)\n",
    "train_data, valid_data, test_data, _ = raw_data\n",
    "\n",
    "batch_size = 32\n",
    "num_steps = 50\n",
    "vocab_size = 10000\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "xi = tf.placeholder(\"float32\",[None,num_steps],name='x_input')\n",
    "yi = tf.placeholder(\"float32\",[None,num_steps],name='y-truth')\n",
    "x = tf.expand_dims(xi,axis=2)\n",
    "y = yi#tf.expand_dims(yi,axis=2)\n",
    "\n",
    "#CONVOLUTIONAL ENCODER\n",
    "\n",
    "\n",
    "with tf.variable_scope('conv1') as scope:\n",
    "    W1 = weight_variable([3,1,100])\n",
    "    b1 = bias_variable([100])       \n",
    "    preacti1 = tf.nn.bias_add(tf.nn.conv1d(x,W1,1,padding='SAME'),b1)\n",
    "    conv1 = activation(preacti1,name=scope.name)\n",
    "\n",
    "with tf.variable_scope('fc') as scope:\n",
    "    Wfc = weight_variable([num_steps*100,num_steps])\n",
    "    bfc = bias_variable([num_steps])\n",
    "    h_flat = tf.reshape(conv1, [-1,num_steps*100])\n",
    "    out = tf.nn.bias_add(tf.matmul(h_flat,Wfc),bfc)\n",
    "    output = tf.reshape(out,(-1,num_steps,1))\n",
    "    \n",
    "with tf.variable_scope('cross_entropy') as scope:    \n",
    "    \n",
    "    logits = tf.nn.softmax(out)\n",
    "    \n",
    "    cross_entropy = tf.reduce_mean(-tf.reduce_sum(y* tf.log(logits), reduction_indices=[1]))\n",
    "\n",
    "   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##Â Loss and metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.name_scope('loss') as scope:\n",
    "    cost = tf.exp(cross_entropy)\n",
    "    tf.summary.scalar('loss',cost)\n",
    "\n",
    "global_step = tf.Variable(0, trainable=False)\n",
    "learning_rate = tf.train.exponential_decay(_learning_rate, global_step,10000, 0.99, staircase=True)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost,global_step=global_step)\n",
    "\n",
    "merged = tf.summary.merge_all()\n",
    "writer = tf.summary.FileWriter(_train_folder+'/cnn', sess.graph)\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "#initialization\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "#launch graph\n",
    "sess.run(init)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Launch Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def run_epoch(session, data,voc_size,verbose=False):\n",
    "    \"\"\"Runs the model on the given data.\"\"\"\n",
    "    epoch_size = ((len(data) // batch_size) - 1) // num_steps\n",
    "    start_time = time.time()\n",
    "    costs = 0.0\n",
    "    iters = 0\n",
    "\n",
    "    for step, (xin, yin) in enumerate(reader.ptb_iterator(data, batch_size,num_steps)):\n",
    "        c, _ = session.run([cost,optimizer],{xi: xin,yi: yin})\n",
    "        costs += c\n",
    "        iters += num_steps\n",
    "\n",
    "        if verbose and step % (epoch_size // 10) == 10:\n",
    "            print(\"%.3f perplexity: %.3f speed: %.0f wps\" %(step * 1.0 / epoch_size, costs,iters * batch_size / (time.time() - start_time)))\n",
    "    return costs\n",
    "\n",
    "raw_data = reader.ptb_raw_data(_data_folder)\n",
    "train_data, valid_data, test_data, voc_size = raw_data\n",
    "\n",
    "for i in range(_epochs):\n",
    "    train_perplexity = run_epoch(sess, train_data,voc_size,verbose=True)\n",
    "    print(\"Epoch: %d Train Perplexity: %.3f\" % (i + 1, train_perplexity))\n",
    "    valid_perplexity = run_epoch(sess, valid_data,voc_size,verbose=True)\n",
    "    print(\"Epoch: %d Valid Perplexity: %.3f\" % (i + 1, valid_perplexity))\n",
    "\n",
    "test_perplexity = run_epoch(sess, test_data,voc_size)\n",
    "print(\"Test Perplexity: %.3f\" % test_perplexity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
